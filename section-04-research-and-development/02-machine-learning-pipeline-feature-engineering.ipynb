{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline - Feature Engineering\n",
    "\n",
    "In the following notebooks, we will go through the implementation of each one of the steps in the Machine Learning Pipeline. \n",
    "\n",
    "We will discuss:\n",
    "\n",
    "1. Data Analysis\n",
    "2. **Feature Engineering**\n",
    "3. Feature Selection\n",
    "4. Model Training\n",
    "5. Obtaining Predictions / Scoring\n",
    "\n",
    "\n",
    "We will use the house price dataset available on [Kaggle.com](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data). See below for more details.\n",
    "\n",
    "===================================================================================================\n",
    "\n",
    "## Predicting Sale Price of Houses\n",
    "\n",
    "The aim of the project is to build a machine learning model to predict the sale price of homes based on different explanatory variables describing aspects of residential houses.\n",
    "\n",
    "\n",
    "### Why is this important? \n",
    "\n",
    "Predicting house prices is useful to identify fruitful investments, or to determine whether the price advertised for a house is over or under-estimated.\n",
    "\n",
    "\n",
    "### What is the objective of the machine learning model?\n",
    "\n",
    "We aim to minimise the difference between the real price and the price estimated by our model. We will evaluate model performance with the:\n",
    "\n",
    "1. mean squared error (mse)\n",
    "2. root squared of the mean squared error (rmse)\n",
    "3. r-squared (r2).\n",
    "\n",
    "\n",
    "### How do I download the dataset?\n",
    "\n",
    "- Visit the [Kaggle Website](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).\n",
    "\n",
    "- Remember to **log in**\n",
    "\n",
    "- Scroll down to the bottom of the page, and click on the link **'train.csv'**, and then click the 'download' blue button towards the right of the screen, to download the dataset.\n",
    "\n",
    "- The download the file called **'test.csv'** and save it in the directory with the notebooks.\n",
    "\n",
    "\n",
    "**Note the following:**\n",
    "\n",
    "-  You need to be logged in to Kaggle in order to download the datasets.\n",
    "-  You need to accept the terms and conditions of the competition to download the dataset\n",
    "-  If you save the file to the directory with the jupyter notebook, then you can run the code as it is written here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility: Setting the seed\n",
    "\n",
    "With the aim to ensure reproducibility between runs of the same notebook, but also between the research and production environment, for each step that includes some element of randomness, it is extremely important that we **set the seed**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to handle datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for the yeo-johnson transformation\n",
    "import scipy.stats as stats\n",
    "\n",
    "# to divide train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# to save the trained scaler class\n",
    "import joblib\n",
    "\n",
    "# to visualise al the columns in the dataframe\n",
    "pd.pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('../data/house_prices/train.csv', index_col='Id')\n",
    "\n",
    "# rows and columns of the data\n",
    "print(data.shape)\n",
    "\n",
    "# visualise the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separate dataset into train and test\n",
    "\n",
    "It is important to separate our data intro training and testing set. \n",
    "\n",
    "When we engineer features, some techniques learn parameters from data. It is important to learn these parameters only from the train set. This is to avoid over-fitting.\n",
    "\n",
    "Our feature engineering techniques will learn:\n",
    "\n",
    "- mean\n",
    "- mode\n",
    "- exponents for the yeo-johnson\n",
    "- category frequency\n",
    "- and category to number mappings\n",
    "\n",
    "from the train set.\n",
    "\n",
    "**Separating the data into train and test involves randomness, therefore, we need to set the seed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's separate into train and test set\n",
    "# Remember to set the seed (random_state for this sklearn function)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(['SalePrice'], axis=1), # predictive variables\n",
    "    data['SalePrice'], # target\n",
    "    test_size=0.1, # portion of dataset to allocate to test set\n",
    "    random_state=0, # we are setting the seed here\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "In the following cells, we will engineer the variables of the House Price Dataset so that we tackle:\n",
    "\n",
    "1. Missing values\n",
    "2. Temporal variables\n",
    "3. Non-Gaussian distributed variables\n",
    "4. Categorical variables: remove rare labels\n",
    "5. Categorical variables: convert strings to numbers\n",
    "5. Put the variables in a similar scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target\n",
    "\n",
    "We apply the logarithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log(y_train)\n",
    "y_test = np.log(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "\n",
    "### Categorical variables\n",
    "\n",
    "We will replace missing values with the string \"missing\" in those variables with a lot of missing data. \n",
    "\n",
    "Alternatively, we will replace missing data with the most frequent category in those variables that contain fewer observations without values. \n",
    "\n",
    "This is common practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's identify the categorical variables\n",
    "# we will capture those of type object\n",
    "\n",
    "cat_vars = [var for var in data.columns if data[var].dtype == 'O']\n",
    "\n",
    "# MSSubClass is also categorical by definition, despite its numeric values\n",
    "# (you can find the definitions of the variables in the data_description.txt\n",
    "# file available on Kaggle, in the same website where you downloaded the data)\n",
    "\n",
    "# lets add MSSubClass to the list of categorical variables\n",
    "cat_vars = cat_vars + ['MSSubClass']\n",
    "\n",
    "# cast all variables as categorical\n",
    "X_train[cat_vars] = X_train[cat_vars].astype('O')\n",
    "X_test[cat_vars] = X_test[cat_vars].astype('O')\n",
    "\n",
    "# number of categorical variables\n",
    "len(cat_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make a list of the categorical variables that contain missing values\n",
    "\n",
    "cat_vars_with_na = [\n",
    "    var for var in cat_vars\n",
    "    if X_train[var].isnull().sum() > 0\n",
    "]\n",
    "\n",
    "# print percentage of missing values per variable\n",
    "X_train[cat_vars_with_na ].isnull().mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables to impute with the string missing\n",
    "with_string_missing = [\n",
    "    var for var in cat_vars_with_na if X_train[var].isnull().mean() > 0.1]\n",
    "\n",
    "# variables to impute with the most frequent category\n",
    "with_frequent_category = [\n",
    "    var for var in cat_vars_with_na if X_train[var].isnull().mean() < 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_string_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values with new label: \"Missing\"\n",
    "\n",
    "X_train[with_string_missing] = X_train[with_string_missing].fillna('Missing')\n",
    "X_test[with_string_missing] = X_test[with_string_missing].fillna('Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in with_frequent_category:\n",
    "    \n",
    "    # there can be more than 1 mode in a variable\n",
    "    # we take the first one with [0]    \n",
    "    mode = X_train[var].mode()[0]\n",
    "    \n",
    "    print(var, mode)\n",
    "    \n",
    "    X_train[var].fillna(mode, inplace=True)\n",
    "    X_test[var].fillna(mode, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we have no missing information in the engineered variables\n",
    "\n",
    "X_train[cat_vars_with_na].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that test set does not contain null values in the engineered variables\n",
    "\n",
    "[var for var in cat_vars_with_na if X_test[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical variables\n",
    "\n",
    "To engineer missing values in numerical variables, we will:\n",
    "\n",
    "- add a binary missing indicator variable\n",
    "- and then replace the missing values in the original variable with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's identify the numerical variables\n",
    "\n",
    "num_vars = [\n",
    "    var for var in X_train.columns if var not in cat_vars and var != 'SalePrice'\n",
    "]\n",
    "\n",
    "# number of numerical variables\n",
    "len(num_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list with the numerical variables that contain missing values\n",
    "vars_with_na = [\n",
    "    var for var in num_vars\n",
    "    if X_train[var].isnull().sum() > 0\n",
    "]\n",
    "\n",
    "# print percentage of missing values per variable\n",
    "X_train[vars_with_na].isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values as we described above\n",
    "\n",
    "for var in vars_with_na:\n",
    "\n",
    "    # calculate the mean using the train set\n",
    "    mean_val = X_train[var].mean()\n",
    "    \n",
    "    print(var, mean_val)\n",
    "\n",
    "    # add binary missing indicator (in train and test)\n",
    "    X_train[var + '_na'] = np.where(X_train[var].isnull(), 1, 0)\n",
    "    X_test[var + '_na'] = np.where(X_test[var].isnull(), 1, 0)\n",
    "\n",
    "    # replace missing values by the mean\n",
    "    # (in train and test)\n",
    "    X_train[var].fillna(mean_val, inplace=True)\n",
    "    X_test[var].fillna(mean_val, inplace=True)\n",
    "\n",
    "# check that we have no more missing values in the engineered variables\n",
    "X_train[vars_with_na].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that test set does not contain null values in the engineered variables\n",
    "\n",
    "[var for var in vars_with_na if X_test[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the binary missing indicator variables\n",
    "\n",
    "X_train[['LotFrontage_na', 'MasVnrArea_na', 'GarageYrBlt_na']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal variables\n",
    "\n",
    "### Capture elapsed time\n",
    "\n",
    "We learned in the previous notebook, that there are 4 variables that refer to the years in which the house or the garage were built or remodeled. \n",
    "\n",
    "We will capture the time elapsed between those variables and the year in which the house was sold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elapsed_years(df, var):\n",
    "    # capture difference between the year variable\n",
    "    # and the year in which the house was sold\n",
    "    df[var] = df['YrSold'] - df[var]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n",
    "    X_train = elapsed_years(X_train, var)\n",
    "    X_test = elapsed_years(X_test, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we drop YrSold\n",
    "X_train.drop(['YrSold'], axis=1, inplace=True)\n",
    "X_test.drop(['YrSold'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical variable transformation\n",
    "\n",
    "### Logarithmic transformation\n",
    "\n",
    "In the previous notebook, we observed that the numerical variables are not normally distributed.\n",
    "\n",
    "We will transform with the logarightm the positive numerical variables in order to get a more Gaussian-like distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in [\"LotFrontage\", \"1stFlrSF\", \"GrLivArea\"]:\n",
    "    X_train[var] = np.log(X_train[var])\n",
    "    X_test[var] = np.log(X_test[var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that test set does not contain null values in the engineered variables\n",
    "[var for var in [\"LotFrontage\", \"1stFlrSF\", \"GrLivArea\"] if X_test[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for train set\n",
    "[var for var in [\"LotFrontage\", \"1stFlrSF\", \"GrLivArea\"] if X_train[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yeo-Johnson transformation\n",
    "\n",
    "We will apply the Yeo-Johnson transformation to LotArea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the yeo-johnson transformation learns the best exponent to transform the variable\n",
    "# it needs to learn it from the train set: \n",
    "X_train['LotArea'], param = stats.yeojohnson(X_train['LotArea'])\n",
    "\n",
    "# and then apply the transformation to the test set with the same\n",
    "# parameter: see who this time we pass param as argument to the \n",
    "# yeo-johnson\n",
    "X_test['LotArea'] = stats.yeojohnson(X_test['LotArea'], lmbda=param)\n",
    "\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check absence of na in the train set\n",
    "[var for var in X_train.columns if X_train[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check absence of na in the test set\n",
    "[var for var in X_train.columns if X_test[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarize skewed variables\n",
    "\n",
    "There were a few variables very skewed, we would transform those into binary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed = [\n",
    "    'BsmtFinSF2', 'LowQualFinSF', 'EnclosedPorch',\n",
    "    '3SsnPorch', 'ScreenPorch', 'MiscVal'\n",
    "]\n",
    "\n",
    "for var in skewed:\n",
    "    \n",
    "    # map the variable values into 0 and 1\n",
    "    X_train[var] = np.where(X_train[var]==0, 0, 1)\n",
    "    X_test[var] = np.where(X_test[var]==0, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables\n",
    "\n",
    "### Apply mappings\n",
    "\n",
    "These are variables which values have an assigned order, related to quality. For more information, check Kaggle website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-map strings to numbers, which determine quality\n",
    "\n",
    "qual_mappings = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5, 'Missing': 0, 'NA': 0}\n",
    "\n",
    "qual_vars = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n",
    "             'HeatingQC', 'KitchenQual', 'FireplaceQu',\n",
    "             'GarageQual', 'GarageCond',\n",
    "            ]\n",
    "\n",
    "for var in qual_vars:\n",
    "    X_train[var] = X_train[var].map(qual_mappings)\n",
    "    X_test[var] = X_test[var].map(qual_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exposure_mappings = {'No': 1, 'Mn': 2, 'Av': 3, 'Gd': 4}\n",
    "\n",
    "var = 'BsmtExposure'\n",
    "\n",
    "X_train[var] = X_train[var].map(exposure_mappings)\n",
    "X_test[var] = X_test[var].map(exposure_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finish_mappings = {'Missing': 0, 'NA': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}\n",
    "\n",
    "finish_vars = ['BsmtFinType1', 'BsmtFinType2']\n",
    "\n",
    "for var in finish_vars:\n",
    "    X_train[var] = X_train[var].map(finish_mappings)\n",
    "    X_test[var] = X_test[var].map(finish_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garage_mappings = {'Missing': 0, 'NA': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}\n",
    "\n",
    "var = 'GarageFinish'\n",
    "\n",
    "X_train[var] = X_train[var].map(garage_mappings)\n",
    "X_test[var] = X_test[var].map(garage_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fence_mappings = {'Missing': 0, 'NA': 0, 'MnWw': 1, 'GdWo': 2, 'MnPrv': 3, 'GdPrv': 4}\n",
    "\n",
    "var = 'Fence'\n",
    "\n",
    "X_train[var] = X_train[var].map(fence_mappings)\n",
    "X_test[var] = X_test[var].map(fence_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check absence of na in the train set\n",
    "[var for var in X_train.columns if X_train[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Rare Labels\n",
    "\n",
    "For the remaining categorical variables, we will group those categories that are present in less than 1% of the observations. That is, all values of categorical variables that are shared by less than 1% of houses, well be replaced by the string \"Rare\".\n",
    "\n",
    "To learn more about how to handle categorical variables visit our course [Feature Engineering for Machine Learning](https://www.udemy.com/course/feature-engineering-for-machine-learning/?referralCode=A855148E05283015CF06) in Udemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture all quality variables\n",
    "\n",
    "qual_vars  = qual_vars + finish_vars + ['BsmtExposure','GarageFinish','Fence']\n",
    "\n",
    "# capture the remaining categorical variables\n",
    "# (those that we did not re-map)\n",
    "\n",
    "cat_others = [\n",
    "    var for var in cat_vars if var not in qual_vars\n",
    "]\n",
    "\n",
    "len(cat_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_frequent_labels(df, var, rare_perc):\n",
    "    \n",
    "    # function finds the labels that are shared by more than\n",
    "    # a certain % of the houses in the dataset\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    tmp = df.groupby(var)[var].count() / len(df)\n",
    "\n",
    "    return tmp[tmp > rare_perc].index\n",
    "\n",
    "\n",
    "for var in cat_others:\n",
    "    \n",
    "    # find the frequent categories\n",
    "    frequent_ls = find_frequent_labels(X_train, var, 0.01)\n",
    "    \n",
    "    print(var, frequent_ls)\n",
    "    print()\n",
    "    \n",
    "    # replace rare categories by the string \"Rare\"\n",
    "    X_train[var] = np.where(X_train[var].isin(\n",
    "        frequent_ls), X_train[var], 'Rare')\n",
    "    \n",
    "    X_test[var] = np.where(X_test[var].isin(\n",
    "        frequent_ls), X_test[var], 'Rare')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of categorical variables\n",
    "\n",
    "Next, we need to transform the strings of the categorical variables into numbers. \n",
    "\n",
    "We will do it so that we capture the monotonic relationship between the label and the target.\n",
    "\n",
    "To learn more about how to encode categorical variables visit our course [Feature Engineering for Machine Learning](https://www.udemy.com/course/feature-engineering-for-machine-learning/?referralCode=A855148E05283015CF06) in Udemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function will assign discrete values to the strings of the variables,\n",
    "# so that the smaller value corresponds to the category that shows the smaller\n",
    "# mean house sale price\n",
    "\n",
    "def replace_categories(train, test, y_train, var, target):\n",
    "    \n",
    "    tmp = pd.concat([X_train, y_train], axis=1)\n",
    "    \n",
    "    # order the categories in a variable from that with the lowest\n",
    "    # house sale price, to that with the highest\n",
    "    ordered_labels = tmp.groupby([var])[target].mean().sort_values().index\n",
    "\n",
    "    # create a dictionary of ordered categories to integer values\n",
    "    ordinal_label = {k: i for i, k in enumerate(ordered_labels, 0)}\n",
    "    \n",
    "    print(var, ordinal_label)\n",
    "    print()\n",
    "\n",
    "    # use the dictionary to replace the categorical strings by integers\n",
    "    train[var] = train[var].map(ordinal_label)\n",
    "    test[var] = test[var].map(ordinal_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for var in cat_others:\n",
    "    replace_categories(X_train, X_test, y_train, var, 'SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check absence of na in the train set\n",
    "[var for var in X_train.columns if X_train[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check absence of na in the test set\n",
    "[var for var in X_test.columns if X_test[var].isnull().sum() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let me show you what I mean by monotonic relationship\n",
    "# between labels and target\n",
    "\n",
    "def analyse_vars(train, y_train, var):\n",
    "    \n",
    "    # function plots median house sale price per encoded\n",
    "    # category\n",
    "    \n",
    "    tmp = pd.concat([X_train, np.log(y_train)], axis=1)\n",
    "    \n",
    "    tmp.groupby(var)['SalePrice'].median().plot.bar()\n",
    "    plt.title(var)\n",
    "    plt.ylim(2.2, 2.6)\n",
    "    plt.ylabel('SalePrice')\n",
    "    plt.show()\n",
    "    \n",
    "for var in cat_others:\n",
    "    analyse_vars(X_train, y_train, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The monotonic relationship is particularly clear for the variables MSZoning and Neighborhood. Note how, the higher the integer that now represents the category, the higher the mean house sale price.\n",
    "\n",
    "(remember that the target is log-transformed, that is why the differences seem so small)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "For use in linear models, features need to be either scaled. We will scale features to the minimum and maximum values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "#  fit  the scaler to the train set\n",
    "scaler.fit(X_train) \n",
    "\n",
    "# transform the train and test set\n",
    "\n",
    "# sklearn returns numpy arrays, so we wrap the\n",
    "# array with a pandas dataframe\n",
    "\n",
    "X_train = pd.DataFrame(\n",
    "    scaler.transform(X_train),\n",
    "    columns=X_train.columns\n",
    ")\n",
    "\n",
    "X_test = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_train.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now save the train and test sets for the next notebook!\n",
    "\n",
    "X_train.to_csv('../data/house_prices/xtrain.csv', index=False)\n",
    "X_test.to_csv('../data/house_prices/xtest.csv', index=False)\n",
    "\n",
    "y_train.to_csv('../data/house_prices/ytrain.csv', index=False)\n",
    "y_test.to_csv('../data/house_prices/ytest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's save the scaler\n",
    "\n",
    "joblib.dump(scaler, 'minmax_scaler.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "That concludes the feature engineering section.\n",
    "\n",
    "# Additional Resources\n",
    "\n",
    "- [Feature Engineering for Machine Learning](https://www.udemy.com/course/feature-engineering-for-machine-learning/?referralCode=A855148E05283015CF06) - Online Course\n",
    "- [Packt Feature Engineering Cookbook](https://www.packtpub.com/data/python-feature-engineering-cookbook) - Book\n",
    "- [Feature Engineering for Machine Learning: A comprehensive Overview](https://trainindata.medium.com/feature-engineering-for-machine-learning-a-comprehensive-overview-a7ad04c896f8) - Article\n",
    "- [Practical Code Implementations of Feature Engineering for Machine Learning with Python](https://towardsdatascience.com/practical-code-implementations-of-feature-engineering-for-machine-learning-with-python-f13b953d4bcd) - Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68c567f59a204e71c57419cad97c63d2d5e33855c150612fd09c6c249abb1d05"
  },
  "kernelspec": {
   "display_name": "feml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "583px",
    "left": "0px",
    "right": "1324px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
